phase:
  id: "PHASE-014"
  slug: "strategic-learning"
  title: "Module 6 Stage 2 & 3 - Strategic Learning & Auto Re-evaluation"

  summary: |
    Tự động thu thập data từ GA/GSC, correlational analysis, alerting,
    và strategy re-evaluation engine để tự học và cải thiện.

  goal: |
    Full strategic learning loop với auto data collection, analysis,
    alerting và strategy re-evaluation.

  from_master_spec:
    sections:
      - "Page 9-10: Module 6 - Giai đoạn 2 & 3"
      - "Giai đoạn 2: Data Ingestor - tự động gọi API GA & GSC"
      - "Giai đoạn 2: Dashboard Phân tích với biểu đồ"
      - "Giai đoạn 2: Correlational Analysis"
      - "Giai đoạn 3: Alerting - tự động báo performance issues"
      - "Giai đoạn 3: Strategy Re-evaluation Engine - tự tạo ContentPlan mới"

  scope:
    included:
      - "Google Analytics 4 API integration"
      - "Google Search Console API integration"
      - "Automated daily data sync jobs"
      - "Correlational analysis (model vs performance)"
      - "Alerting system (low performance detection)"
      - "Strategy re-evaluation engine"
      - "Auto content plan regeneration"
      - "Cost vs rank analysis"
    excluded:
      - "Advanced ML models cho prediction"
      - "Real-time analytics"

  dependencies:
    - "PHASE-013"
    - "PHASE-010"
    - "PHASE-005"

  inputs:
    - name: "Analytics Service from PHASE-013"
      type: "codebase"
      location: "services/analytics/"

  outputs:
    - name: "GA/GSC Connectors"
      type: "code"
      location_hint: "services/analytics/connectors/"
    - name: "Analysis Engine"
      type: "code"
      location_hint: "services/analytics/analyzer.py"
    - name: "Strategy Re-evaluator"
      type: "code"
      location_hint: "services/strategy-evaluator/"

  implementation_prompt: |
    Triển khai **PHASE-014 – Strategic Learning Engine**.

    YÊU CẦU:

    1. **Google Analytics 4 Connector**:
    ```python
    from google.analytics.data_v1beta import BetaAnalyticsDataClient
    from google.analytics.data_v1beta.types import RunReportRequest
    
    class GA4Connector:
        def __init__(self, property_id: str):
            self.client = BetaAnalyticsDataClient()
            self.property_id = property_id
        
        async def get_page_metrics(self, start_date: str, end_date: str):
            request = RunReportRequest(
                property=f"properties/{self.property_id}",
                dimensions=[{"name": "pagePath"}],
                metrics=[
                    {"name": "screenPageViews"},
                    {"name": "sessions"},
                    {"name": "averageSessionDuration"}
                ],
                date_ranges=[{
                    "start_date": start_date,
                    "end_date": end_date
                }]
            )
            
            response = self.client.run_report(request)
            return self._parse_response(response)
    ```

    2. **Google Search Console Connector**:
    ```python
    from googleapiclient.discovery import build
    
    class GSCConnector:
        def __init__(self, site_url: str):
            self.service = build('searchconsole', 'v1', 
                               credentials=self.get_credentials())
            self.site_url = site_url
        
        async def get_performance_data(self, start_date: str, end_date: str):
            request = {
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': ['page', 'query'],
                'rowLimit': 25000
            }
            
            response = self.service.searchanalytics().query(
                siteUrl=self.site_url,
                body=request
            ).execute()
            
            return response.get('rows', [])
    ```

    3. **Automated Data Sync Job**:
    ```python
    @celery_app.task
    async def sync_analytics_daily():
        """Daily job to sync GA and GSC data."""
        workspaces = await get_active_workspaces()
        
        for ws in workspaces:
            sites = await get_sites(ws.id)
            
            for site in sites:
                # Sync GA4
                ga4 = GA4Connector(site.ga_property_id)
                ga_data = await ga4.get_page_metrics(
                    start_date=(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                    end_date=datetime.now().strftime('%Y-%m-%d')
                )
                
                # Sync GSC
                gsc = GSCConnector(site.url)
                gsc_data = await gsc.get_performance_data(
                    start_date=(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                    end_date=datetime.now().strftime('%Y-%m-%d')
                )
                
                # Insert into ClickHouse
                await insert_performance_data(ga_data, gsc_data)
                
                # Update PostgreSQL summary
                await update_article_performance_sync(ws.id)
    ```

    4. **Correlational Analysis**:
    ```python
    class PerformanceAnalyzer:
        async def analyze_model_performance(self, workspace_id: str):
            """Analyze which AI models perform best."""
            query = """
            SELECT
              ai_model_used,
              avg(position) as avg_position,
              sum(clicks) as total_clicks,
              avg(cost_usd) as avg_cost
            FROM fact_performance
            WHERE workspace_id = %(workspace_id)s
              AND date >= today() - 30
            GROUP BY ai_model_used
            """
            
            results = await clickhouse_client.execute(query, {
                'workspace_id': workspace_id
            })
            
            # Example output:
            # GPT-4o: avg_position=8.5, avg_cost=0.05
            # GPT-3.5: avg_position=12.3, avg_cost=0.01
            # => GPT-4o performs 30% better despite 5x cost
            
            return results
    ```

    5. **Alerting System**:
    ```python
    class AlertingEngine:
        async def check_performance_issues(self, workspace_id: str):
            # Check for clusters not performing well
            query = """
            SELECT
              c.name as cluster_name,
              avg(f.position) as avg_position,
              sum(f.clicks) as total_clicks
            FROM fact_performance f
            JOIN articles a ON f.article_id = a.id
            JOIN content_plans cp ON a.plan_id = cp.id
            JOIN topic_clusters c ON cp.cluster_id = c.id
            WHERE f.workspace_id = %(workspace_id)s
              AND f.date >= today() - 30
            GROUP BY c.id, c.name
            HAVING avg_position > 20 OR total_clicks < 10
            """
            
            issues = await clickhouse_client.execute(query, {
                'workspace_id': workspace_id
            })
            
            for issue in issues:
                await send_alert(workspace_id, {
                    'type': 'low_performance',
                    'cluster': issue['cluster_name'],
                    'avg_position': issue['avg_position'],
                    'message': f"Cluster '{issue['cluster_name']}' has avg position {issue['avg_position']}"
                })
    ```

    6. **Strategy Re-evaluation Engine**:
    ```python
    class StrategyReEvaluator:
        async def re_evaluate_cluster(self, cluster_id: str):
            """
            Phát hiện: Cluster 'XYZ' (dùng Prompt_A, Model_Gemini1.0)
            đã 30 ngày không rank.
            
            Hành động: Tự động tạo ContentPlan mới với chiến lược mới.
            """
            cluster = await get_cluster(cluster_id)
            performance = await get_cluster_performance(cluster_id, days=30)
            
            if performance['avg_position'] > 20:
                # Poor performance detected
                
                # Analyze what's working for competitors
                best_practices = await analyze_competitors(cluster)
                
                # Generate new content plan with different strategy
                new_plan = await create_content_plan({
                    'cluster_id': cluster_id,
                    'title': f"[Rewrite] {cluster.name}",
                    'priority': 'high',
                    'notes': f"Re-evaluation due to poor performance. New strategy: {best_practices}",
                    'ai_model': 'gpt-4o',  # Upgrade to better model
                    'prompt_template': 'detailed_v2'  # Different prompt
                })
                
                # Emit event
                await publish_event('content.plan.regenerate', {
                    'cluster_id': cluster_id,
                    'new_plan_id': new_plan.id,
                    'reason': 'poor_performance_30d'
                })
    ```

    7. **Scheduled Re-evaluation Job**:
    ```python
    @celery_app.task
    async def weekly_strategy_evaluation():
        """Weekly job to re-evaluate underperforming clusters."""
        workspaces = await get_active_workspaces()
        
        for ws in workspaces:
            await strategy_evaluator.evaluate_workspace(ws.id)
    ```

    KẾT QUẢ:
    - Auto data sync từ GA & GSC
    - Performance analysis tự động
    - Alerts gửi khi có issues
    - Strategy re-evaluation tự tạo plans mới

  acceptance_criteria:
    - "GA4 connector sync data thành công"
    - "GSC connector sync data thành công"
    - "Daily sync jobs chạy tự động"
    - "Correlational analysis chính xác"
    - "Alerts được gửi khi detect poor performance"
    - "Strategy re-evaluator tạo new content plans"
    - "Event content.plan.regenerate được publish"
    - "Tests pass"

  non_goals:
    - "Advanced ML prediction models"
    - "Real-time analytics dashboards"

  notes:
    - "GA4 và GSC credentials cần setup trong workspace settings"
    - "Daily sync job nên chạy lúc 2AM để tránh peak hours"
    - "Re-evaluation không nên quá aggressive (max 1 lần/tuần per cluster)"
